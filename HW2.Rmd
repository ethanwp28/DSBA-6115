---
title: "HW2"
author: "Ethan Pinto"
date: "`r Sys.Date()`"
output: html_document
---

From Introduction to Statistical Learning in R:

Chapter 2, Problem 1

a)	We would expect the performance of a flexible statistical learning method to be better when the sample size n is extremely large, and the number of predictors p is small. With a large sample size, a flexible statistical model will be better equipped to fit the large data set.

b)	The performance of the flexible model would be worse when the number of predictors is extremely large and the number of observations is small. This is because a flexible models generally perform better when there the data sets are larger, thus in this case, the model could potentially overfit the data.

c)	When the relationship between the predictors and their response is highly non-linear, we would expect the flexible model to perform better. Since flexible models have more degrees of freedom to approximate a non-linear relationship, the flexible model will perform better on a non-linear dataset.

d)	When the variance of the error terms is extremely high, we would expect the flexible model to perform worse, as the model would likely overfit because as the noise level is higher due to the high variance error terms, the model would likely fit this noise. An inflexible model would be less likely to fit the error noise. The data points will be far from the ideal function that describes the data if the variance of the error terms is very high. This would indicate that the function is linear thus a simpler model would be a better solution to estimate.


Chapter 2, Problem 2

a)	This is a regression problem since the result is quantitative. Since we are measuring the impact of the predictors on the CEO’s salary, we are most interested in inference. Sample size n = 500, predictors p are profit, number of employees, and industry.

b)	Since the result is binary, (success or failure) then this is a classification problem, and we are most interested in prediction. Sample size n = 20, predictors p are price, marketing budget, competition price, and the other ten variables not mentioned.

c)	We are looking for a quantitative value (percentage change in USD/Euro exchange rate) thus this is a regression problem. We are most interested in a prediction. Sample size n is 52 (52 weeks in a year). Predictors p are percent change in US market, percent change in UK market, and percent change in the German market.


Chapter 2, Problem 7

```{r}
## Part a)
library(dplyr)
training_data <- data.frame(
  Obs = 1:6,
  X1 = c(0, 2, 0, 0, -1, 1),
  X2 = c(3, 0, 1, 1, 0, 1),
  X3 = c(0, 0, 3, 2, 1, 1),
  Y = c("Red", "Red", "Red", "Green", "Green", "Red")
)
test_point <- data.frame(X1 = 0, X2 = 0, X3 = 0)
euclidean_distance <- function(a, b) {
  sqrt(sum((a - b)^2))
}
distances <- apply(training_data[, c("X1", "X2", "X3")], 1, function(row) 
{
  euclidean_distance(row, test_point)
})
training_data <- training_data %>%
  mutate(Distance = distances)
print(training_data)

## Part b)
training_data <- data.frame(
  Obs = 1:6,
  X1 = c(0, 2, 0, 0, -1, 1),
  X2 = c(3, 0, 1, 1, 0, 1),
  X3 = c(0, 0, 3, 2, 1, 1),
  Y = c("Red", "Red", "Red", "Green", "Green", "Red")
)

test_point <- c(X1 = 0, X2 = 0, X3 = 0)

euclidean_distance <- function(a, b) {
  sqrt(sum((a - b)^2))
}

distances <- apply(training_data[, c("X1", "X2", "X3")], 1, function(row) {
  euclidean_distance(row, test_point)
})

training_data$Distance <- distances

nearest_obs <- training_data[which.min(training_data$Distance),]
prediction <- nearest_obs$Y
print(paste("The prediction with K=1 is:", prediction)) 

## Part c)
training_data <- data.frame(
  Obs = 1:6,
  X1 = c(0, 2, 0, 0, -1, 1),
  X2 = c(3, 0, 1, 1, 0, 1),
  X3 = c(0, 0, 3, 2, 1, 1),
  Y = c("Red", "Red", "Red", "Green", "Green", "Red")
)

test_point <- c(X1 = 0, X2 = 0, X3 = 0)

euclidean_distance <- function(a, b) {
  sqrt(sum((a - b)^2))
}

distances <- apply(training_data[, c("X1", "X2", "X3")], 1, function(row) {
  euclidean_distance(row, test_point)
})

training_data$Distance <- distances

top_k <- training_data[order(training_data$Distance), ][1:3,]
prediction <- as.character(names(sort(table(top_k$Y), decreasing = TRUE)[1]))
print("Top 3 neighbors are:")
print(top_k)
print(paste("The prediction with K=3 is:", prediction))
```

Chapter 3, Problem 3

From the given, the least square formula: Salary = 50 + 20 * GPA + .07 * IQ + 35 * Level + .01 * (GPA * IQ) - 10 * (GPA * Level)

a)	Option iii is correct. To compare the average starting salaries of high school and college graduates for a fixed value of IQ and GPA, look at the term influence by Level: 35 * Level – 10 * (GPA * Level). High school graduates (Level = 0) Contribution to Salary is: 35 * 0 – 10 * GPA * 0 = 0. College graduates (Level = 1) contribution to Salary is: 35 * 1 – 10 * GPA * 1 = 35 – 10 * GPA.
We can deduce that for GPA >/= to 3.5, the term will be negative, and thus high school grads would earn more; whereas when GPA </= to 3.5, college graduates would earn more. Thus, the correct answer is option iii.

b)	Substituting in to our least square formula, we have: Salary = 50 + 20 * 4.0 + .07 * 110 + 35 * 1 + .01 * (4.0 * 110) -  10 * (4.0 * 1) = 137.1. Thus, the salary of a college graduate with an IQ of 110 and a GPA of 4.0 is $137,100.

c)	False. The size of the coefficient for the interaction term in and of itself is not enough evidence to conclude whether there is a meaningful interaction effect or not. To determine the presence of an interaction effect, statistical tests like hypothesis testing or confidence intervals for the coefficient can be used. Even a small coefficient can be statistically significant if the sample size is large or if the predictor variables have a meaningful range. On the other hand, a large coefficient may not be significant if there is a lot of variability in the data or if the sample size is small.

Chapter 3, Problem 9

```{r}
## (a) Produce a scatterplot matrix which includes all of the variables
## in the data set.

library(ISLR)
data(Auto)

pairs(Auto)

## (b) Compute the matrix of correlations between the variables using
## the function cor(). You will need to exclude the name variable, cor() which is qualitative.

cor_matrix <- cor(Auto[, -which(names(Auto) == "name")])

print(cor_matrix)

## (c) Use the lm() function to perform a multiple linear regression
## with mpg as the response and all other variables except name as
## the predictors. Use the summary() function to print the results.
## Comment on the output. For instance:
##  i. Is there a relationship between the predictors and the response?
##  ii. Which predictors appear to have a statistically significant
##      relationship to the response?
##  iii. What does the coefficient for the year variable suggest?

fit <- lm(mpg ~ . - name, data = Auto)

summary(fit)

## (d) Use the plot() function to produce diagnostic plots of the linear
## regression fit. Comment on any problems you see with the fit.
## Do the residual plots suggest any unusually large outliers? Does
## the leverage p

plot(fit)

## (e) Use the * and : symbols to fit linear regression models with
## interaction effects. Do any interactions appear to be statistically
## significant?

## using *
fit_interaction_star <- lm(mpg ~ . - name + horsepower * weight, data = Auto)

summary(fit_interaction_star)

## using :
fit_interaction_colon <- lm(mpg ~ . - name + horsepower:weight, data = Auto)

summary(fit_interaction_colon)

## using multiple interaction effects

fit_multiple_interactions <- lm(mpg ~ . - name + horsepower:weight + displacement:cylinders, data = Auto)

summary(fit_multiple_interactions)

## (f) Try a few different transformations of the variables, such as
## log(X), √X, X2. Comment on your findings.

## log transformation
fit_log <- lm(mpg ~ . - name + log(horsepower), data = Auto)

summary(fit_log)

## square root transformation
fit_sqrt <- lm(mpg ~ . - name + sqrt(displacement), data = Auto)

summary(fit_sqrt)

## squared transformation

fit_square <- lm(mpg ~ . - name + I(weight^2), data = Auto)

summary(fit_square)
```

c) 
i. Is there a relationship between the predictors and the response? The p-value: < 2.2e-16 for a F-statistic of 252.4. This shows a statistically significant relationship between the predictors and the mpg.

ii. Which predictors appear to have a statistically significant relationship to the response? Displacement, weight, year and origin are statistically significant as their p-values are below 0.05 or near zero.

iii. What does the coefficient for the year variable suggest? The coefficient for the 'year' predictor is ~ 0.75, and suggests that increasing it by one year will mean a vehicles predicted mpg will be 0.75mpg higher, assuming all other predictors are kept constant. Thus, we can say that newer cars have better mpg.

d) The chart indicates some non-linearity in the data, and so polynomial regression could provide a better fit than simple linear regression. The residual plots suggest a few large outliers, namely, points 323, 326, and 327. The leverage plot also indicates a large outlier, point 14.

e) Based on the generated, the model with the best interaction is the 3rd model, between cylinders and displacement.

f) The log transformation seems to be the best of the three models here as it has the highest R2 when the F statistic is statistically significant.

Chapter 3, Problem 13

```{r}
set.seed(1)

## (a) Using the rnorm() function, create a vector, x, containing 100
## observations drawn from a N(0, 1) distribution. This represents
## a feature, X.

x <- rnorm(100, mean = 0, sd = 1)

head(x) ## for verification

## (b) Using the rnorm() function, create a vector, eps, containing 100
## observations drawn from a N(0, 0.25) distribution—a normal
## distribution with mean zero and variance 0.25.

eps <- rnorm(100, mean = 0, sd = sqrt(0.25))

head(eps)

## (c) Using x and eps, generate a vector y according to the model
## Y = −1+0.5X + ϵ. (3.39)
## What is the length of the vector y? What are the values of β0
## and β1 in this linear model?

y <- -1 + 0.5 * x + eps

head(y)

length_y <- length(y)

print(paste("The length of y is:", length_y))

## (d) Create a scatterplot displaying the relationship between x and
## y. Comment on what you observe.

plot(x, y, main = "Scatterplot of x vs y",
     xlab = "x", ylab = "y", pch = 19, col = "blue")

## (e) Fit a least squares linear model to predict y using x. Comment
## on the model obtained. How do βˆ0 and βˆ1 compare to β0 and
## β1?

fit_model <- lm(y ~ x)

summary(fit_model)

## (f) Display the least squares line on the scatterplot obtained in (d).
## Draw the population regression line on the plot, in a different
## color. Use the legend() command to create an appropriate legend.

plot(x, y, main = "Scatterplot with Least Squares & Population Regression Lines",
     xlab = "x", ylab = "y", pch = 19, col = "blue")

abline(fit_model, col = "red", lwd = 2)

abline(a = -1, b = 0.5, col = "green", lwd = 2)

legend("topright", legend = c("Least Squares Line", "Population Line"),
       col = c("red", "green"), lwd = 2)

## (g) Now fit a polynomial regression model that predicts y using x
## and x2. Is there evidence that the quadratic term improves the
## model fit? Explain your answer.

poly_model <- lm(y ~ x + I(x^2))

summary(poly_model)

## (h) Repeat (a)–(f) after modifying the data generation process in
## such a way that there is less noise in the data. The model (3.39)
## should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term
## ϵ in (b). Describe your results.
set.seed(1)

x <- rnorm(100)

eps <- rnorm(100, mean = 0, sd = 0.1) ## decreased standard deviation to 0.1

y <- -1 + 0.5*x + eps

length(y)

plot(x, y, main = "Scatterplot with Less Noise",
     xlab = "x", ylab = "y", pch = 19, col = "blue")

fit_model_less_noise <- lm(y ~ x)
summary(fit_model_less_noise)

abline(fit_model_less_noise, col = "red", lwd = 2)
abline(a = -1, b = 0.5, col = "green", lwd = 2)

legend("topright", legend = c("Least Squares Line", "True Line"),
       col = c("red", "green"), lwd = 2)

poly_model_less_noise <- lm(y ~ x + I(x^2))
summary(poly_model_less_noise)

## (i) Repeat (a)–(f) after modifying the data generation process in
## such a way that there is more noise in the data. The model
## (3.39) should remain the same. You can do this by increasing
## the variance of the normal distribution used to generate the
## error term ϵ in (b). Describe your results.

x <- rnorm(100)

eps <- rnorm(100, mean = 0, sd = 0.5)  ## increased standard deviation to 0.5

y <- -1 + 0.5*x + eps

length(y)

plot(x, y, main = "Scatterplot with More Noise",
     xlab = "x", ylab = "y", pch = 19, col = "blue")

fit_model_more_noise <- lm(y ~ x)
summary(fit_model_more_noise)

abline(fit_model_more_noise, col = "red", lwd = 2)

abline(a = -1, b = 0.5, col = "green", lwd = 2)

legend("topright", legend = c("Least Squares Line", "True Line"),
       col = c("red", "green"), lwd = 2)

poly_model_more_noise <- lm(y ~ x + I(x^2))
summary(poly_model_more_noise)

## (j) What are the confidence intervals for β0 and β1 based on the
## original data set, the noisier dataset, and the less noisy data
## set? Comment on your results.

## original data set
x_original <- rnorm(100)
eps_original <- rnorm(100, 0, 0.25)
y_original <- -1 + 0.5*x_original + eps_original

fit_original <- lm(y_original ~ x_original)

## noisier data set

x_noisy <- rnorm(100)
eps_noisy <- rnorm(100, 0, 0.5)
y_noisy <- -1 + 0.5*x_noisy + eps_noisy

fit_noisy <- lm(y_noisy ~ x_noisy)

## less noisy data set

x_less_noisy <- rnorm(100)
eps_less_noisy <- rnorm(100, 0, 0.1)
y_less_noisy <- -1 + 0.5*x_less_noisy + eps_less_noisy

fit_less_noisy <- lm(y_less_noisy ~ x_less_noisy)

## confidence intervals

confint(fit_original)
confint(fit_noisy)
confint(fit_less_noisy)
```

c) Length of the vector y is 100. β0 = -1 and β1 = 0.5

d) There is a positive relationship between x and y.

e) The coefficients obtained are very similar to the original model, which is expected because both models are linear regressions. The difference between β1 and ^ β1 is only .00053 and the difference between β0 and ^ β0 is only .01885.

g) Both t statistic and F statistic of x2  are not statistically significant; in addition R2 does not improve significantly increase. As a result, we can deduce that there is not enough evidence to support the quadratic term improves the model’s fit. Also, we already pointed out earlier that the relationship between x and y is linear, not quadratic, so it goes without saying that the addition of a quadratic term will not help the model significantly.

h) The overall results are very similar to the original model. When we reduce noise we expect to see a more condensed scatterplot, and the linear relationship is reinforced, which are confirmed by the results. The reduction of noise means the reduction of error, thus it follows that R2 is also increased.

g) Based on what we found above, we’d expect that the opposite results will occur when we introduce more noise into the model. Our results confirm this expectation, showing a much wider data spread and variance. The coefficients are not significant at 1%, and R2 is decreased.

